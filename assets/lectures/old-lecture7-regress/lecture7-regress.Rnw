%% beamer/knitr slides 
%% for Statistical Modeling and Data Visualization course @ UMass
%% Nicholas Reich: nick [at] schoolph.umass.edu


\documentclass[table]{beamer}


\input{../../slide-includes/standard-knitr-beamer-preamble}

%	The following variables are assumed by the standard preamble:
%	Global variable containing module name:
\title{Regression Miscellany}
%	Global variable containing module shortname:
%		(Currently unused, may be used in future.)
\newcommand{\ModuleShortname}{introRegression}
%	Global variable containing author name:
\author{Nicholas G Reich}
%	Global variable containing text of license terms:
\newcommand{\LicenseText}{Made available under the Creative Commons Attribution-ShareAlike 3.0 Unported License: http://creativecommons.org/licenses/by-sa/3.0/deed.en\textunderscore US }
%	Instructor: optional, can leave blank.
%		Recommended format: {Instructor: Jane Doe}
\newcommand{\Instructor}{}
%	Course: optional, can leave blank.
%		Recommended format: {Course: Biostatistics 101}
\newcommand{\Course}{}


% leftovers from openintro slides

% pr: left and right parentheses
\newcommand{\pr}[1]{
\left( #1 \right)
}

\xdefinecolor{rubineRed}{rgb}{0.89,0,0.30}
\newcommand{\red}[1]{\textit{\textcolor{rubineRed}{#1}}}

\xdefinecolor{irishGreen}{rgb}{0,0.60,0}	
\newcommand{\green}[1]{\textit{\textcolor{irishGreen}{#1}}}


\xdefinecolor{hlblue}{rgb}{0.051,0.65,1}
\newcommand{\hl}[1]{\textit{\textcolor{hlblue}{#1}}}



\input{../../slide-includes/shortcuts}

\hypersetup{colorlinks,linkcolor=,urlcolor=MainColor}


%	******	Document body begins here	**********************

\begin{document}

%	Title page
\begin{frame}[plain]
	\titlepage
\end{frame}

%	******	Everything through the above line must be placed at
%		the top of any TeX file using the statsTeachR standard
%		beamer preamble. 





%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{frame}{Outline}

\bi
  \myitem Review of model syntax
  \myitem Summary of $R^2$ and $R^2_{adj}$
  \myitem Outlier classification and influence
  \myitem Model selection
  %\myitem Categorical variable representation via dummy variables
\ei
	
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{frame}

\centering
\Huge
model syntax

\end{frame}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
 
 
 
\begin{frame}[fragile]{Example data}

 

 
\bi

 
    \myitem D = a quantitative variable

 
    \myitem A = a quantitative variable

 
    \myitem G = a categorical variable with two levels, S and K

 
\ei

 

 
<<simData, tidy=FALSE, echo=FALSE, message=FALSE, fig.height=4>>=
library(ggplot2)
library(dplyr)
theme_set(theme_bw())

n=20
A <- seq(0, 10, length.out = n)
G <- rep(c("S", "K"), each=n/2)
b0 = 5; b1 = 2; b2 = 12; b3 = -3

 
D <- b0 + b1*A + b2*(G=="S") + b3*A*(G=="S") + rnorm(n)

 
m <- data_frame(A, G, D)

 
p <- ggplot(m, aes(x=A, y=D, color=factor(G))) + geom_text(aes(label=as.character(G))) + theme(legend.position="none") + scale_color_manual(values=c("#e41a1c", "#377eb8"))

 
p

 
@

 

 

 
\end{frame}

 

 
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

 

 
\begin{frame}[fragile]{Draw the model...}

 

 
\bi

 
    \myitem D = a quantitative variable

 
    \myitem A = a quantitative variable

 
    \myitem G = a categorical variable with two levels, S and K

 
\ei

 

 
<<simData4, tidy=FALSE, echo=FALSE, message=FALSE, fig.height=4>>=

 
p + annotate("text", x=2, y=23, label="D ~ 1", size=10)

 
@

 

 
\end{frame}

 

 
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

 

 
\begin{frame}[fragile]{Draw the model...}

 

 
\bi

 
    \myitem D = a quantitative variable

 
    \myitem A = a quantitative variable

 
    \myitem G = a categorical variable with two levels, S and K

 
\ei

 

 
<<simData5, tidy=FALSE, echo=FALSE, message=FALSE, fig.height=4>>=

 
p + annotate("text", x=2, y=23, label="D ~ G", size=10)

 
@

 

 
\end{frame}

 
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

 

 
\begin{frame}[fragile]{Draw the model...}

 

 
\bi

 
    \myitem D = a quantitative variable

 
    \myitem A = a quantitative variable

 
    \myitem G = a categorical variable with two levels, S and K

 
\ei

 

 
<<simData1, tidy=FALSE, echo=FALSE, message=FALSE, fig.height=4>>=

 
p + annotate("text", x=2, y=23, label="D ~ A", size=10)

 
@

 

 
\end{frame}

 

 

 
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

 

 
\begin{frame}[fragile]{Draw the model...}

 

 
\bi

 
    \myitem D = a quantitative variable

 
    \myitem A = a quantitative variable

 
    \myitem G = a categorical variable with two levels, S and K

 
\ei

 

 
<<simData2, tidy=FALSE, echo=FALSE, message=FALSE, fig.height=4>>=

 
p + annotate("text", x=2, y=23, label="D ~ A - 1", size=10)

 
@

 

 
\end{frame}

 

 
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

 

 
\begin{frame}[fragile]{Draw the model...}

 

 
\bi

 
    \myitem D = a quantitative variable

 
    \myitem A = a quantitative variable

 
    \myitem G = a categorical variable with two levels, S and K

 
\ei

 

 
<<simData3, tidy=FALSE, echo=FALSE, message=FALSE, fig.height=4>>=

 
p + annotate("text", x=2, y=23, label="D ~ A + G", size=10)

 
@

 

 
\end{frame}

 

 

 

 
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

 

 
\begin{frame}[fragile]{Draw the model...}

 

 
\bi

 
    \myitem D = a quantitative variable

 
    \myitem A = a quantitative variable

 
    \myitem G = a categorical variable with two levels, S and K

 
\ei

 

 
<<simData6, tidy=FALSE, echo=FALSE, message=FALSE, fig.height=4>>=

 
p + annotate("text", x=2, y=23, label="D ~ A*G", size=10)

 
@

 

 
\end{frame}

 

 
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

 

 
\begin{frame}[fragile]{Draw the model...}

 

 
\bi

 
    \myitem D = a quantitative variable

 
    \myitem A = a quantitative variable

 
    \myitem G = a categorical variable with two levels, S and K

 
\ei

 

 
<<simData7, tidy=FALSE, echo=FALSE, message=FALSE, fig.height=4>>=

 
p + annotate("text", x=2, y=23, label="D ~ poly(A, 2)", size=10)

 
@

 

 
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{frame}

\centering
\Huge
summary of $R^2$ and $R^2_{adj}$

\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{frame}[fragile]
\frametitle{Another look at $R^2$}

$R^2$ can be calculated in three ways:

\pause

\begin{enumerate}

\item square the correlation coefficient of $x$ and $y$ {\small (how we have been calculating it)}

\pause

\item square the correlation coefficient of $y$ and $\hat{y}$

\pause

\item based on definition: 

\[ R^2 = \frac{\text{explained variability in }y}{\text{total variability in }y} \]

\end{enumerate}

\pause

Using an \hl{ANOVA} table we can calculate the explained variability and total variability in $y$.

\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{frame}[fragile]
\frametitle{Sum of squares}

\vspace{-0.5cm}

{\small
\begin{center}
\begin{tabular}{lrrrrr}
  \hline
 & Df & Sum Sq & Mean Sq & F value & Pr($>$F) \\ 
  \hline
female\_house & 1 & 132.57 & 132.57 & 18.68 & 0.00 \\ 
  Residuals & 49 & 347.68 & 7.10 &  &  \\ 
   \hline
Total & 50 & 480.25 \\
   \hline
\end{tabular}
\end{center}
}


\begin{eqnarray*}
\text{Sum of squares of $y$: } SS_{Total} &=& \sum(y - \bar{y})^2 = 480.25 \\
%\text{Variance of $y$: } V(y) &=&  \frac{\sum(y - \bar{y})^2}{n-1} = 9.6 \\
\text{Sum of squares of residuals: } SS_{Error} &=& \sum e_i^2 = 347.68  \\
\text{Sum of squares of $x$: } SS_{Model} &=& SS_{Total} - SS_{Error}  \\
&=& 480.25 - 347.68 = 132.57
\end{eqnarray*}


\[ R^2 = \frac{\text{explained variability}}{\text{total variability}} = \frac{132.57}{480.25} = 0.28 \]

\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{frame}
\frametitle{Why bother?}

Why bother with another approach for calculating $R^2$ when we had a perfectly good way to calculate it as the correlation coefficient squared?

\pause

\begin{itemize}
\item For single-predictor linear regression, having three ways to calculate the same value may seem like overkill. 
\item However, in multiple linear regression, we can't calculate $R^2$ as the square of the correlation between $x$ and $y$ because we have multiple $x$s. 
\item And next we'll learn another measure of explained variability, \hl{adjusted $R^2$}, that requires the use of the third approach, ratio of explained and unexplained variability. 
\end{itemize}

\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{frame}
\frametitle{Predicting poverty using \% female hh + \% white}


\begin{center}
\begin{tabular}{rrrrr}
  \hline
\hl{Linear model:}&  Estimate & Std. Error & t value & Pr($>$$|$t$|$) \\ 
  \hline
(Intercept) & -2.58 & 5.78 & -0.45 & 0.66 \\ 
  female\_house & 0.89 & 0.24 & 3.67 & 0.00 \\ 
  white & 0.04 & 0.04 & 1.08 & 0.29 \\ 
   \hline
\end{tabular}
\end{center}

\vspace{0.1cm}

\begin{center}
\begin{tabular}{lrrrrr}
  \hline
\hl{ANOVA:} & Df & Sum Sq & Mean Sq & F value & Pr($>$F) \\ 
  \hline
female\_house & 1 & 132.57 & 132.57 & 18.74 & 0.00 \\ 
  white & 1 & 8.21 & 8.21 & 1.16 & 0.29 \\ 
  Residuals & 48 & 339.47 & 7.07 &  &  \\ 
   \hline
Total & 50 &    480.25\\
   \hline
\end{tabular}
\end{center}

\pause

\[ R^2 = \frac{\text{explained variability}}{\text{total variability}} = \frac{132.57 + 8.21}{480.25} = 0.29 \]

\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{frame}

Does adding the variable \var{white} to the model add valuable information that wasn't provided by \var{female\_house}?

\begin{center}
\includegraphics[width=0.8\textwidth]{../lecture6-mlr/8-1_intro_mlr/figures/poverty/poverty}
\end{center}

\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\begin{frame}[fragile]
\frametitle{$R^2$ vs. adjusted $R^2$}

\renewcommand\arraystretch{1.25}
\begin{center}
\begin{tabular}{l | c  c}
			& $R^2$	& Adjusted $R^2$ \\
\hline
Model 1 (Single-predictor)	& 0.28	& 0.26 \\
Model 2 (Multiple)			& 0.29	& 0.26 	
\end{tabular}
\end{center}

\pause

\begin{itemize}

\item When \underline{any} variable is added to the model $R^2$ increases.

\pause

\item But if the added variable doesn't really provide any new information, or is completely unrelated, adjusted $R^2$ does not increase.

\end{itemize}



\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{frame}
\frametitle{Adjusted $R^2$}

\begin{block}{Adjusted $R^2$}
{\[ R^2_{adj} = 1 - \pr{ \frac{ SS_{Error} }{ SS_{Total} } \times \frac{n - 1}{n - p - 1} } \]
where $n$ is the number of cases and $p$ is the number of predictors (explanatory variables) in the model.}

\end{block}

\begin{itemize}

\item Because $p$ is never negative, $R^2_{adj}$ will always be smaller than $R^2$.

\item $R^2_{adj}$ applies a penalty for the number of predictors included in the model.

\item Therefore, we choose models with higher $R^2_{adj}$ over others.

\end{itemize}

\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{frame}
\frametitle{Calculate adjusted $R^2$}

\begin{center}
\begin{tabular}{lrrrrr}
  \hline
\hl{ANOVA:} & Df & Sum Sq & Mean Sq & F value & Pr($>$F) \\ 
  \hline
female\_house & 1 & 132.57 & 132.57 & 18.74 & 0.0001 \\ 
  white & 1 & 8.21 & 8.21 & 1.16 & 0.2868 \\ 
  Residuals & 48 & 339.47 & 7.07 &  &  \\ 
   \hline
Total & 50 &    480.25\\
   \hline
\end{tabular}
\end{center}

\begin{eqnarray*}
R^2_{adj} &=& 1 - \pr{ \frac{ SS_{Error} }{ SS_{Total} } \times \frac{n - 1}{n - p - 1} } \\
\pause
&=& 1 - \pr{ \frac{ 339.47 }{ 480.25 } \times \frac{51 - 1}{51 - 2 - 1} }   \\
\pause
&=& 1- \pr{ \frac{ 339.47 }{ 480.25 } \times \frac{50}{48} } \\
\pause
&=& 1 -  0.74 \\
\pause
&=& 0.26
\end{eqnarray*}

\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{frame}

\centering
\Huge
types of outliers

 \end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{frame}
\frametitle{Some terminology}
 
\begin{itemize}

\item \hl{Outliers} are points that lie away from the cloud  of points.

\item Outliers that lie horizontally away from the center of the cloud are called \hl{high leverage} points.

\item High leverage points that actually influence the \underline{slope} of the regression line are called \hl{influential} points.

\item In order to determine if a point is influential, visualize the regression line with and without the point. Does the slope of the line change considerably? If so, then the point is influential. If not, then it's not an influential point.

\end{itemize}

\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{frame}{Influence}

Intuitively, ``influence'' is a combination of outlying-ness and leverage. More specifically, we can measure the ``deletion influence'' of each observation: quantify how much $\hat\bbeta$ changes if an observation is left out.
\bi
	\myitem Mathematically: $|\hat{\bbeta}- \hat{\bbeta}_{(-i)}|$
	\myitem Cook's distance is a value we can calculate for each observation in our dataset that measures this deletion influence. (It uses some nice tricks of linear algebra without having to refit the regression iteratively without each point.)
\ei

\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{frame}[fragile]{Example diagnostic plots with poverty data}
<<echo=FALSE>>=
poverty = read.table("../lecture5-slr/7-1_linefit_res_corr/figures/poverty/poverty.txt", 
                     h = T, sep = "\t")
names(poverty) = c("state", "metro_res", "white", "hs_grad", "poverty", "female_house")
@

You can use the {\tt plot.lm()} function to look at leverage, outlying-ness, and influence all together. 
\scriptsize
<<levPlots1, fig.height=4, tidy=FALSE>>=
mlr = lm(poverty ~ female_house + white, data = poverty)
plot(mlr, which=5)
@

\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{frame}[fragile]{Investigate identified points!}

<<echo=FALSE, fig.height=3>>=
plot(mlr, which=5)
@


\scriptsize
<<plotlm1, fig.height=3>>=
poverty[12,]
colMeans(poverty[,2:6])
@

\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{frame}[fragile]{Model checking summary}

\begin{block}{You are looking for...}
\bi
 \myitem Points that show worrisome level of influence $\implies$ sensitivity analysis!
 \myitem Systematic departures from model assumptions $\implies$ transformations, different model structure
 \myitem Unrealistic outliers $\implies$ check your data!
\ei
\end{block}

No points show worrisome influence in this poverty data analysis, although observation 12 was high leverage. 


\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



\begin{frame}

\centering
\Huge
model selection

\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



\begin{frame}{Model selection}

\begin{block}{Why are you building a model in the first place?}
\end{block}

\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{frame}{Model selection: considerations}

\begin{block}{Things to keep in mind...}
\bi
    \myitem {\bf Why am I building a model?} Some common answers
    \bi
        	\item Estimate an association
		\item Test a particular hypothesis
		\item Predict new values
	\ei
	\myitem What predictors will I allow?
    \myitem What predictors are needed?
\ei

Different answers to these questions will yield different final models.

\end{block}

\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\begin{frame}{Model selection: realities}

\centering {\em All models are wrong. Some are more useful than others.} \\ - George Box


\bi
	\myitem In practice, issues with sample size, collinearity, and available predictors are real problems.
	\myitem There is not a single best algorithm for model selection! It pretty much always requires thoughful reasoning and knowledge about the data at hand. 
	\myitem When in doubt (unless you are specifically ``data mining''), err on the side creating a process that does not require choices being made (by you or the computer) about which covariates to include.
\ei
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{frame}{Basic ideas for model selection}

\begin{block}{For association studies, when your sample size is large}
\bi
        \myitem Include key covariates of interest.
        \myitem Include covariates needed because they might be confounders.
        \myitem Include covariates that your colleagues/reviewers/collaborators will demand be included for face validity.
        \myitem Do NOT go on a fishing expedition for significant results!
        \myitem Do NOT use ``stepwise selection'' methods!
        \myitem Subject the selected model to model checking/diagnostics, possibly adjust model structure (i.e. include non-linear relationships with covariates) as needed.
\ei
\end{block}

\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{frame}{Basic ideas for model selection}

\begin{block}{For association studies, when your sample size is small}
\bi
        \myitem Same as above, but may need to be more frugal with how many predictors you include.
        \myitem Rule of thumb for multiple linear regression is to have at least 15 observations for each covariate you include in your model.
\ei
\end{block}


\end{frame}



\end{document}